{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Demo: One-Shot LLM on Raw Data\n",
    "\n",
    "## ⚠️ Anti-Pattern Demonstration\n",
    "\n",
    "**This notebook intentionally demonstrates the WRONG way to use LLMs for analytics.**\n",
    "\n",
    "\n",
    "\"Which channel mix change is most likely to improve CAC next month, given a recent anomaly in referral traffic?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment loaded\n",
      "✓ Database connected\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Connect to database\n",
    "db_path = '../data/synthetic_data.duckdb'\n",
    "conn = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "print(\"✓ Environment loaded\")\n",
    "print(\"✓ Database connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Raw Table Inventory\n",
    "\n",
    "Let's show the LLM our raw tables without any semantic guidance about:\n",
    "- How to join them safely\n",
    "- Which metrics are canonical\n",
    "- What time windows to use\n",
    "- How to attribute revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables:\n",
      "  - dim_adgroups\n",
      "  - dim_campaigns\n",
      "  - dim_creatives\n",
      "  - dim_customers\n",
      "  - dim_products\n",
      "  - fact_ad_spend\n",
      "  - fact_orders\n",
      "  - fact_sessions\n",
      "\n",
      "Database Schema:\n",
      "\n",
      "dim_adgroups:\n",
      "  - adgroup_id (VARCHAR)\n",
      "  - campaign_id (VARCHAR)\n",
      "  - audience (VARCHAR)\n",
      "  - placement (VARCHAR)\n",
      "\n",
      "dim_campaigns:\n",
      "  - campaign_id (VARCHAR)\n",
      "  - channel (VARCHAR)\n",
      "  - campaign_name (VARCHAR)\n",
      "  - start_date (DATE)\n",
      "  - end_date (DATE)\n",
      "  - objective (VARCHAR)\n",
      "\n",
      "dim_creatives:\n",
      "  - creative_id (VARCHAR)\n",
      "  - adgroup_id (VARCHAR)\n",
      "  - format (VARCHAR)\n",
      "  - asset_url (VARCHAR)\n",
      "\n",
      "dim_customers:\n",
      "  - customer_id (VARCHAR)\n",
      "  - first_visit_date (DATE)\n",
      "  - region (VARCHAR)\n",
      "  - loyalty_segment (VARCHAR)\n",
      "  - primary_device (VARCHAR)\n",
      "  - acquisition_channel (VARCHAR)\n",
      "\n",
      "dim_products:\n",
      "  - sku (VARCHAR)\n",
      "  - category (VARCHAR)\n",
      "  - subcategory (VARCHAR)\n",
      "  - brand (VARCHAR)\n",
      "  - price (DOUBLE)\n",
      "  - margin_pct (DOUBLE)\n",
      "  - margin (DOUBLE)\n",
      "\n",
      "fact_ad_spend:\n",
      "  - date (DATE)\n",
      "  - campaign_id (VA...\n"
     ]
    }
   ],
   "source": [
    "# Get table schemas\n",
    "tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "print(\"Available tables:\")\n",
    "for table in tables:\n",
    "    print(f\"  - {table[0]}\")\n",
    "\n",
    "# Build raw schema description for LLM\n",
    "schema_description = \"Database Schema:\\n\\n\"\n",
    "\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    columns = conn.execute(f\"DESCRIBE {table_name}\").fetchall()\n",
    "    schema_description += f\"{table_name}:\\n\"\n",
    "    for col in columns:\n",
    "        schema_description += f\"  - {col[0]} ({col[1]})\\n\"\n",
    "    schema_description += \"\\n\"\n",
    "\n",
    "# Show a preview\n",
    "print(\"\\n\" + schema_description[:800] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: One-Shot Prompt Execution\n",
    "\n",
    "Now we'll ask the LLM to write SQL to analyze the business question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "================================================================================\n",
      "To analyze the channel mix change that is most likely to improve Customer Acquisition Cost (CAC) next month, especially considering a recent anomaly in referral traffic, we can examine the ad spend and the performance of different campaigns and channels over the past month. We will calculate the CAC for each channel and then identify which channel has the potential for improvement based on the performance metrics such as impressions, clicks, and conversions.\n",
      "\n",
      "Here is the SQL query that addresses this analysis:\n",
      "\n",
      "```sql\n",
      "WITH channel_performance AS (\n",
      "    SELECT \n",
      "        c.channel,\n",
      "        SUM(fas.spend) AS total_spend,\n",
      "        SUM(fas.impressions) AS total_impressions,\n",
      "        SUM(fas.clicks) AS total_clicks,\n",
      "        SUM(fa.pages_viewed) AS total_pages_viewed,\n",
      "        SUM(CASE WHEN fa.converted_flag THEN 1 ELSE 0 END) AS total_conversions,\n",
      "        SUM(fa.revenue) AS total_revenue\n",
      "    FROM \n",
      "        fact_ad_spend fas\n",
      "    JOIN \n",
      "        dim_campaigns c ON fas.campaign_id = c.campaign_id\n",
      "    JOIN \n",
      "        fact_sessions fa ON fas.campaign_id = fa.campaign_id \n",
      "                          AND fas.adgroup_id = fa.adgroup_id \n",
      "                          AND fas.creative_id = fa.creative_id\n",
      "    WHERE \n",
      "        fas.date >= DATE_TRUNC('month', CURRENT_DATE) - INTERVAL '1 month' \n",
      "        AND fas.date < DATE_TRUNC('month', CURRENT_DATE)\n",
      "        AND c.channel <> 'referral' -- Exclude referral channel\n",
      "    GROUP BY \n",
      "        c.channel\n",
      "),\n",
      "cac_calculation AS (\n",
      "    SELECT \n",
      "        channel,\n",
      "        total_spend,\n",
      "        total_conversions,\n",
      "        CASE \n",
      "            WHEN total_conversions = 0 THEN NULL \n",
      "            ELSE total_spend / total_conversions \n",
      "        END AS cac\n",
      "    FROM \n",
      "        channel_performance\n",
      ")\n",
      "SELECT \n",
      "    channel, \n",
      "    cac,\n",
      "    total_spend,\n",
      "    total_conversions,\n",
      "    (total_spend / NULLIF(total_conversions, 0)) AS potential_improvement -- Calculate potential improvement\n",
      "FROM \n",
      "    cac_calculation\n",
      "ORDER BY \n",
      "    cac ASC\n",
      "LIMIT 1; -- Get the channel with the lowest CAC for potential improvement\n",
      "```\n",
      "\n",
      "### Explanation of the SQL Query:\n",
      "1. **Channel Performance Calculation**: \n",
      "   - The first CTE (`channel_performance`) aggregates the total ad spend, impressions, clicks, pages viewed, and conversions for each channel over the last month, excluding the referral channel due to the anomaly.\n",
      "\n",
      "2. **CAC Calculation**:\n",
      "   - The second CTE (`cac_calculation`) computes the Customer Acquisition Cost (CAC) for each channel based on total spend and conversions.\n",
      "\n",
      "3. **Final Selection**:\n",
      "   - The main query selects the channel with the lowest CAC, which is the most promising candidate for improvement next month, along with the relevant metrics.\n",
      "\n",
      "This query provides insights into which channel might be optimized to enhance the efficiency of customer acquisition, especially in light of the referral traffic anomaly.\n"
     ]
    }
   ],
   "source": [
    "business_question = \"\"\"Which channel mix change is most likely to improve CAC next month, \n",
    "given a recent anomaly in referral traffic?\"\"\"\n",
    "\n",
    "prompt = f\"\"\"{schema_description}\n",
    "\n",
    "Business Question: {business_question}\n",
    "\n",
    "You have just gotten a schema description for a set of tables in our warehouse with descriptive column names.\n",
    "\n",
    "Write a single SQL query to analyze this question and provide a recommendation.\n",
    "\n",
    "Return your response in this format:\n",
    "SQL:\n",
    "[your SQL query]\n",
    "\"\"\"\n",
    "\n",
    "# Call LLM\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llm_output = response.choices[0].message.content\n",
    "print(\"LLM Response:\")\n",
    "print(\"=\" * 80)\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and execute the SQL\n",
    "import re\n",
    "\n",
    "# Parse SQL from response\n",
    "sql_match = re.search(r'SQL:(.*?)(?:Explanation:|$)', llm_output, re.DOTALL)\n",
    "if sql_match:\n",
    "    generated_sql = sql_match.group(1).strip()\n",
    "    # Remove markdown code blocks if present\n",
    "    generated_sql = re.sub(r'^```sql\\s*', '', generated_sql)\n",
    "    generated_sql = re.sub(r'^```\\s*', '', generated_sql)\n",
    "    generated_sql = re.sub(r'```\\s*$', '', generated_sql)\n",
    "    generated_sql = generated_sql.strip()\n",
    "    \n",
    "    print(\"\\nExecuting generated SQL...\\n\")\n",
    "    print(generated_sql)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        result_df = conn.execute(generated_sql).df()\n",
    "        print(\"\\nQuery Results:\")\n",
    "        print(result_df)\n",
    "        \n",
    "        # Store for failure analysis\n",
    "        bad_results = result_df\n",
    "        bad_sql = generated_sql\n",
    "    except Exception as e:\n",
    "        print(f\"\\nQuery failed: {e}\")\n",
    "        bad_results = None\n",
    "        bad_sql = generated_sql\n",
    "else:\n",
    "    print(\"Could not parse SQL from LLM response\")\n",
    "    bad_results = None\n",
    "    bad_sql = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Failure Exhibit\n",
    "\n",
    "Let's analyze what went wrong with the LLM-generated SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for specific failure patterns\n",
    "failures = []\n",
    "\n",
    "if bad_sql:\n",
    "    sql_lower = bad_sql.lower()\n",
    "    \n",
    "    # Failure 1: Revenue attribution error\n",
    "    if 'fact_orders' in sql_lower and 'fact_sessions' not in sql_lower:\n",
    "        failures.append({\n",
    "            'failure': 'Revenue Attribution Error',\n",
    "            'description': 'Joined fact_orders directly to dim_campaigns without going through fact_sessions',\n",
    "            'impact': 'Revenue is incorrectly attributed; many orders will be dropped or duplicated',\n",
    "            'correct': 'Must use: fact_orders → fact_sessions → dim_campaigns (last-touch attribution)'\n",
    "        })\n",
    "    \n",
    "    # Failure 2: Many-to-many inflation\n",
    "    if 'fact_sessions' in sql_lower and 'fact_ad_spend' in sql_lower:\n",
    "        if sql_lower.count('join') >= 2 and 'session_id' not in sql_lower:\n",
    "            failures.append({\n",
    "                'failure': 'Many-to-Many Cartesian Explosion',\n",
    "                'description': 'Joined fact_sessions and fact_ad_spend on campaign_id without proper grain',\n",
    "                'impact': 'Row counts multiply incorrectly; metrics are inflated by 10-100x',\n",
    "                'correct': 'These tables must be aggregated separately before joining, or use semantic layer'\n",
    "            })\n",
    "    \n",
    "    # Failure 3: Time window drift\n",
    "    date_filters = re.findall(r'(date|timestamp).*?interval.*?(\\d+)', sql_lower)\n",
    "    if len(set([d[1] for d in date_filters])) > 1:\n",
    "        failures.append({\n",
    "            'failure': 'Time Window Drift',\n",
    "            'description': 'Different tables use different date ranges (e.g., 30 days for spend, 90 days for revenue)',\n",
    "            'impact': 'CAC calculation mixes mismatched time periods; results are meaningless',\n",
    "            'correct': 'All metrics must use the same canonical window (default: 90 days)'\n",
    "        })\n",
    "    \n",
    "    # Failure 4: Metric misuse\n",
    "    if 'order_id' in sql_lower and ('cac' in sql_lower or 'acquisition' in sql_lower):\n",
    "        failures.append({\n",
    "            'failure': 'Metric Definition Error',\n",
    "            'description': 'Used orders in CAC calculation instead of conversions',\n",
    "            'impact': 'CAC is understated because not all sessions convert to orders',\n",
    "            'correct': 'CAC = spend / conversions, where conversions come from fact_sessions.converted_flag'\n",
    "        })\n",
    "    \n",
    "    # Failure 5: Dimension ambiguity\n",
    "    if 'utm_source' in sql_lower or 'source' in sql_lower:\n",
    "        failures.append({\n",
    "            'failure': 'Dimension Ambiguity',\n",
    "            'description': 'Mixed dim_campaigns.channel with utm_source or other dimension',\n",
    "            'impact': 'Channels are inconsistently defined; totals don\\'t match',\n",
    "            'correct': 'Must use canonical dimension: dim_campaigns.channel only'\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FAILURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, failure in enumerate(failures, 1):\n",
    "    print(f\"\\n{i}. {failure['failure']}\")\n",
    "    print(f\"   What happened: {failure['description']}\")\n",
    "    print(f\"   Impact: {failure['impact']}\")\n",
    "    print(f\"   Correct approach: {failure['correct']}\")\n",
    "\n",
    "if not failures:\n",
    "    print(\"\\nNo obvious failures detected in pattern matching, but the results are still likely wrong!\")\n",
    "    print(\"This demonstrates another problem: it's hard to even detect when one-shot LLM SQL fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The Overconfident Narrative\n",
    "\n",
    "Despite these errors, the LLM produced a confident recommendation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract insight\n",
    "insight_match = re.search(r'INSIGHT:(.*)', llm_output, re.DOTALL)\n",
    "if insight_match:\n",
    "    insight = insight_match.group(1).strip()\n",
    "    print(\"LLM Recommendation:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(insight)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n⚠️ This recommendation is based on WRONG DATA due to the failures above.\")\n",
    "    print(\"⚠️ Implementing this could waste budget and harm business performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Post-Mortem Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**Single LLM calls on raw data fail because:**\n",
    "\n",
    "1. **No semantic contracts**: The LLM doesn't know the canonical metric definitions\n",
    "2. **No join validation**: Complex schemas allow many wrong join paths\n",
    "3. **No time window enforcement**: Different metrics drift to different periods\n",
    "4. **No grain management**: Many-to-many joins cause silent data explosions\n",
    "5. **Overconfidence**: The LLM has no way to know it's wrong\n",
    "\n",
    "### The Right Way\n",
    "\n",
    "The solution requires:\n",
    "- **Semantic layer**: Canonical metrics with tested SQL templates\n",
    "- **Modular agents**: Small, focused, testable components\n",
    "- **Deterministic logic**: Use LLMs only for ambiguity resolution\n",
    "- **Observability**: Log every decision and SQL execution\n",
    "- **Testing**: Validate joins, grains, and results\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "See `02_good_modular_dspy.ipynb` for the correct implementation using:\n",
    "- DSPy agent architecture\n",
    "- Semantic layer from `config/semantic.yml`\n",
    "- Reproducible, testable, explainable analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Vague Request with No Acceptance Criteria\n",
    "\n",
    "Let's try another anti-pattern: asking the LLM to \"analyze\" data without:\n",
    "- Specific questions\n",
    "- Clear success criteria\n",
    "- Defined scope\n",
    "- Expected output format\n",
    "- Constraints on what to look for\n",
    "\n",
    "This is common in practice: analysts ask LLMs to \"find something interesting\" or \"analyze our data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a vague, open-ended question\n",
    "vague_question = \"\"\"Analyze our marketing data and tell me something interesting that could help improve performance.\"\"\"\n",
    "\n",
    "vague_prompt = f\"\"\"{schema_description}\n",
    "\n",
    "Task: {vague_question}\n",
    "\n",
    "Look at our marketing data and provide insights. Write SQL queries as needed and explain what you found.\n",
    "\"\"\"\n",
    "\n",
    "# Call LLM with no constraints\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": vague_prompt}],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "vague_output = response.choices[0].message.content\n",
    "print(\"LLM Response to Vague Request:\")\n",
    "print(\"=\" * 80)\n",
    "print(vague_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Wrong with This Approach?\n",
    "\n",
    "Even if the SQL executes, this approach fails because:\n",
    "\n",
    "1. **No Clear Goal**: \"Something interesting\" is subjective - interesting to who? For what purpose?\n",
    "\n",
    "2. **No Validation Criteria**: How do we know if the insight is:\n",
    "   - Correct?\n",
    "   - Actionable?\n",
    "   - Novel?\n",
    "   - Relevant to business goals?\n",
    "\n",
    "3. **No Scope**: The LLM might:\n",
    "   - Look at irrelevant metrics\n",
    "   - Use wrong time windows\n",
    "   - Mix incompatible data\n",
    "   - Generate obvious or useless insights\n",
    "\n",
    "4. **No Output Format**: The response could be:\n",
    "   - A single number without context\n",
    "   - A complex visualization we can't implement\n",
    "   - Multiple contradictory insights\n",
    "   - Generic advice that doesn't use our data\n",
    "\n",
    "5. **Not Reproducible**: Running this again with the same data might produce completely different \"insights\"\n",
    "\n",
    "6. **Not Testable**: How do you write a test for \"tell me something interesting\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the problems\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROBLEMS WITH VAGUE REQUESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "problems = [\n",
    "    {\n",
    "        'problem': 'No Clear Success Criteria',\n",
    "        'example': 'LLM says \"Snapchat performs well\" - is that interesting? Actionable? True?',\n",
    "        'impact': 'Cannot validate if the insight is useful or correct'\n",
    "    },\n",
    "    {\n",
    "        'problem': 'Generic, Obvious Insights',\n",
    "        'example': 'LLM might say \"Channels with lower CAC are more efficient\" (duh!)',\n",
    "        'impact': 'Waste time on insights that provide no value'\n",
    "    },\n",
    "    {\n",
    "        'problem': 'Mixing Time Windows',\n",
    "        'example': 'Compares last month Snapchat to last year Google Search',\n",
    "        'impact': 'Invalid comparisons that mislead decision-making'\n",
    "    },\n",
    "    {\n",
    "        'problem': 'No Prioritization',\n",
    "        'example': 'LLM lists 10 \"interesting\" things without ranking importance',\n",
    "        'impact': 'Cannot decide which insight to act on first'\n",
    "    },\n",
    "    {\n",
    "        'problem': 'Hallucinated Patterns',\n",
    "        'example': 'LLM \"finds\" a trend that is just random noise',\n",
    "        'impact': 'False insights lead to wrong business decisions'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, p in enumerate(problems, 1):\n",
    "    print(f\"\\n{i}. {p['problem']}\")\n",
    "    print(f\"   Example: {p['example']}\")\n",
    "    print(f\"   Impact: {p['impact']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WHAT GOOD LOOKS LIKE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Instead of vague requests, use:\n",
    "\n",
    "✅ SPECIFIC QUESTION:\n",
    "   \"Which channel mix change is most likely to improve CAC next month?\"\n",
    "\n",
    "✅ CLEAR CONSTRAINTS:\n",
    "   - Time window: Last 90 days\n",
    "   - Metrics: CAC, ROAS by channel\n",
    "   - Output: Specific reallocation percentage\n",
    "   - Success criteria: Quantified CAC improvement with confidence interval\n",
    "\n",
    "✅ VALIDATION:\n",
    "   - Check that channels mentioned exist in data\n",
    "   - Verify that percentage is reasonable (3-10%)\n",
    "   - Ensure output includes specific dollar amounts\n",
    "   - Validate that math is correct\n",
    "\n",
    "✅ STRUCTURED OUTPUT:\n",
    "   - Hypothesis: \"Shift X% from [channel] to [channel]\"\n",
    "   - Expected impact: \"$X.XX improvement\"\n",
    "   - Confidence: \"95% CI: [$Y, $Z]\"\n",
    "   - Risks: 2-3 specific risks\n",
    "\n",
    "✅ REPRODUCIBLE:\n",
    "   - Same question → same answer (or within CI)\n",
    "   - Documented methodology\n",
    "   - Versioned semantic definitions\n",
    "   - Logged decision process\n",
    "\n",
    "See 02_good_modular_dspy.ipynb for this approach in action.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Vague vs Specific\n",
    "\n",
    "| Aspect | Vague Request | Specific Request (Good Demo) |\n",
    "|--------|--------------|------------------------------|\n",
    "| **Question** | \"Tell me something interesting\" | \"Which channel mix change improves CAC?\" |\n",
    "| **Success Criteria** | ??? | Quantified CAC improvement with CI |\n",
    "| **Scope** | Entire database, any metric | CAC and ROAS by channel, 90 days |\n",
    "| **Output Format** | Freeform text | Structured JSON with hypothesis |\n",
    "| **Validation** | Impossible | Check against semantic catalog |\n",
    "| **Actionability** | Maybe? | Clear: shift X% budget from A to B |\n",
    "| **Reproducibility** | No | Yes (run_id + versioned specs) |\n",
    "| **Testability** | No | Yes (inline tests) |\n",
    "\n",
    "**Key Insight**: The specificity of the request determines the quality of the output.\n",
    "\n",
    "Vague inputs → vague outputs, even with perfect LLMs.\n",
    "\n",
    "Specific, constrained inputs → actionable, validated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "conn.close()\n",
    "print(\"\\n✓ Demo complete. Database connection closed.\")\n",
    "print(\"\\nRemember: This is what NOT to do. See the good demo for the right approach.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
