{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Demo: One-Shot LLM on Raw Data\n",
    "\n",
    "## ⚠️ WARNING: Anti-Pattern Demonstration\n",
    "\n",
    "**This notebook intentionally demonstrates the WRONG way to use LLMs for analytics.**\n",
    "\n",
    "We will:\n",
    "1. Show raw table schemas to an LLM with no semantic guidance\n",
    "2. Ask it to write SQL and provide insights in a single shot\n",
    "3. Watch it produce confident but completely wrong results\n",
    "\n",
    "### Expected Failure Modes\n",
    "\n",
    "This approach will fail in multiple ways:\n",
    "- **Revenue attribution error**: Wrong join path, attributing revenue incorrectly\n",
    "- **Many-to-many inflation**: Cartesian explosion from improper joins\n",
    "- **Time window drift**: Inconsistent date ranges across metrics\n",
    "- **Metric misuse**: Using orders instead of conversions in CAC calculation\n",
    "- **Dimension ambiguity**: Mixing utm_source and channel, causing duplication\n",
    "\n",
    "### Business Question\n",
    "\n",
    "\"Which channel mix change is most likely to improve CAC next month, given a recent anomaly in referral traffic?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Connect to database\n",
    "db_path = '../data/synthetic_data.duckdb'\n",
    "conn = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "print(\"✓ Environment loaded\")\n",
    "print(\"✓ Database connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Raw Table Inventory\n",
    "\n",
    "Let's show the LLM our raw tables without any semantic guidance about:\n",
    "- How to join them safely\n",
    "- Which metrics are canonical\n",
    "- What time windows to use\n",
    "- How to attribute revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table schemas\n",
    "tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "print(\"Available tables:\")\n",
    "for table in tables:\n",
    "    print(f\"  - {table[0]}\")\n",
    "\n",
    "# Build raw schema description for LLM\n",
    "schema_description = \"Database Schema:\\n\\n\"\n",
    "\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    columns = conn.execute(f\"DESCRIBE {table_name}\").fetchall()\n",
    "    schema_description += f\"{table_name}:\\n\"\n",
    "    for col in columns:\n",
    "        schema_description += f\"  - {col[0]} ({col[1]})\\n\"\n",
    "    schema_description += \"\\n\"\n",
    "\n",
    "# Show a preview\n",
    "print(\"\\n\" + schema_description[:800] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: One-Shot Prompt Execution\n",
    "\n",
    "Now we'll ask the LLM to:\n",
    "1. Write SQL to analyze the business question\n",
    "2. Provide a confident recommendation\n",
    "\n",
    "**No guardrails. No semantic layer. No validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_question = \"\"\"Which channel mix change is most likely to improve CAC next month, \n",
    "given a recent anomaly in referral traffic?\"\"\"\n",
    "\n",
    "prompt = f\"\"\"{schema_description}\n",
    "\n",
    "Business Question: {business_question}\n",
    "\n",
    "Write a SINGLE SQL query to analyze this question and provide a recommendation.\n",
    "The query should calculate CAC by channel and identify opportunities for optimization.\n",
    "\n",
    "Return your response in this format:\n",
    "SQL:\n",
    "[your SQL query]\n",
    "\n",
    "INSIGHT:\n",
    "[your recommendation in 2-3 sentences]\n",
    "\"\"\"\n",
    "\n",
    "# Call LLM\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llm_output = response.choices[0].message.content\n",
    "print(\"LLM Response:\")\n",
    "print(\"=\" * 80)\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and execute the SQL\n",
    "import re\n",
    "\n",
    "# Parse SQL from response\n",
    "sql_match = re.search(r'SQL:(.*?)(?:INSIGHT:|$)', llm_output, re.DOTALL)\n",
    "if sql_match:\n",
    "    generated_sql = sql_match.group(1).strip()\n",
    "    # Remove markdown code blocks if present\n",
    "    generated_sql = re.sub(r'^```sql\\s*', '', generated_sql)\n",
    "    generated_sql = re.sub(r'^```\\s*', '', generated_sql)\n",
    "    generated_sql = re.sub(r'```\\s*$', '', generated_sql)\n",
    "    generated_sql = generated_sql.strip()\n",
    "    \n",
    "    print(\"\\nExecuting generated SQL...\\n\")\n",
    "    print(generated_sql)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        result_df = conn.execute(generated_sql).df()\n",
    "        print(\"\\nQuery Results:\")\n",
    "        print(result_df)\n",
    "        \n",
    "        # Store for failure analysis\n",
    "        bad_results = result_df\n",
    "        bad_sql = generated_sql\n",
    "    except Exception as e:\n",
    "        print(f\"\\nQuery failed: {e}\")\n",
    "        bad_results = None\n",
    "        bad_sql = generated_sql\n",
    "else:\n",
    "    print(\"Could not parse SQL from LLM response\")\n",
    "    bad_results = None\n",
    "    bad_sql = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Failure Exhibit\n",
    "\n",
    "Let's analyze what went wrong with the LLM-generated SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for specific failure patterns\n",
    "failures = []\n",
    "\n",
    "if bad_sql:\n",
    "    sql_lower = bad_sql.lower()\n",
    "    \n",
    "    # Failure 1: Revenue attribution error\n",
    "    if 'fact_orders' in sql_lower and 'fact_sessions' not in sql_lower:\n",
    "        failures.append({\n",
    "            'failure': 'Revenue Attribution Error',\n",
    "            'description': 'Joined fact_orders directly to dim_campaigns without going through fact_sessions',\n",
    "            'impact': 'Revenue is incorrectly attributed; many orders will be dropped or duplicated',\n",
    "            'correct': 'Must use: fact_orders → fact_sessions → dim_campaigns (last-touch attribution)'\n",
    "        })\n",
    "    \n",
    "    # Failure 2: Many-to-many inflation\n",
    "    if 'fact_sessions' in sql_lower and 'fact_ad_spend' in sql_lower:\n",
    "        if sql_lower.count('join') >= 2 and 'session_id' not in sql_lower:\n",
    "            failures.append({\n",
    "                'failure': 'Many-to-Many Cartesian Explosion',\n",
    "                'description': 'Joined fact_sessions and fact_ad_spend on campaign_id without proper grain',\n",
    "                'impact': 'Row counts multiply incorrectly; metrics are inflated by 10-100x',\n",
    "                'correct': 'These tables must be aggregated separately before joining, or use semantic layer'\n",
    "            })\n",
    "    \n",
    "    # Failure 3: Time window drift\n",
    "    date_filters = re.findall(r'(date|timestamp).*?interval.*?(\\d+)', sql_lower)\n",
    "    if len(set([d[1] for d in date_filters])) > 1:\n",
    "        failures.append({\n",
    "            'failure': 'Time Window Drift',\n",
    "            'description': 'Different tables use different date ranges (e.g., 30 days for spend, 90 days for revenue)',\n",
    "            'impact': 'CAC calculation mixes mismatched time periods; results are meaningless',\n",
    "            'correct': 'All metrics must use the same canonical window (default: 90 days)'\n",
    "        })\n",
    "    \n",
    "    # Failure 4: Metric misuse\n",
    "    if 'order_id' in sql_lower and ('cac' in sql_lower or 'acquisition' in sql_lower):\n",
    "        failures.append({\n",
    "            'failure': 'Metric Definition Error',\n",
    "            'description': 'Used orders in CAC calculation instead of conversions',\n",
    "            'impact': 'CAC is understated because not all sessions convert to orders',\n",
    "            'correct': 'CAC = spend / conversions, where conversions come from fact_sessions.converted_flag'\n",
    "        })\n",
    "    \n",
    "    # Failure 5: Dimension ambiguity\n",
    "    if 'utm_source' in sql_lower or 'source' in sql_lower:\n",
    "        failures.append({\n",
    "            'failure': 'Dimension Ambiguity',\n",
    "            'description': 'Mixed dim_campaigns.channel with utm_source or other dimension',\n",
    "            'impact': 'Channels are inconsistently defined; totals don\\'t match',\n",
    "            'correct': 'Must use canonical dimension: dim_campaigns.channel only'\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FAILURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, failure in enumerate(failures, 1):\n",
    "    print(f\"\\n{i}. {failure['failure']}\")\n",
    "    print(f\"   What happened: {failure['description']}\")\n",
    "    print(f\"   Impact: {failure['impact']}\")\n",
    "    print(f\"   Correct approach: {failure['correct']}\")\n",
    "\n",
    "if not failures:\n",
    "    print(\"\\nNo obvious failures detected in pattern matching, but the results are still likely wrong!\")\n",
    "    print(\"This demonstrates another problem: it's hard to even detect when one-shot LLM SQL fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The Overconfident Narrative\n",
    "\n",
    "Despite these errors, the LLM produced a confident recommendation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract insight\n",
    "insight_match = re.search(r'INSIGHT:(.*)', llm_output, re.DOTALL)\n",
    "if insight_match:\n",
    "    insight = insight_match.group(1).strip()\n",
    "    print(\"LLM Recommendation:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(insight)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n⚠️ This recommendation is based on WRONG DATA due to the failures above.\")\n",
    "    print(\"⚠️ Implementing this could waste budget and harm business performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Post-Mortem Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**Single LLM calls on raw data fail because:**\n",
    "\n",
    "1. **No semantic contracts**: The LLM doesn't know the canonical metric definitions\n",
    "2. **No join validation**: Complex schemas allow many wrong join paths\n",
    "3. **No time window enforcement**: Different metrics drift to different periods\n",
    "4. **No grain management**: Many-to-many joins cause silent data explosions\n",
    "5. **Overconfidence**: The LLM has no way to know it's wrong\n",
    "\n",
    "### The Right Way\n",
    "\n",
    "The solution requires:\n",
    "- **Semantic layer**: Canonical metrics with tested SQL templates\n",
    "- **Modular agents**: Small, focused, testable components\n",
    "- **Deterministic logic**: Use LLMs only for ambiguity resolution\n",
    "- **Observability**: Log every decision and SQL execution\n",
    "- **Testing**: Validate joins, grains, and results\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "See `02_good_modular_dspy.ipynb` for the correct implementation using:\n",
    "- DSPy agent architecture\n",
    "- Semantic layer from `config/semantic.yml`\n",
    "- Reproducible, testable, explainable analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "conn.close()\n",
    "print(\"\\n✓ Demo complete. Database connection closed.\")\n",
    "print(\"\\nRemember: This is what NOT to do. See the good demo for the right approach.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
