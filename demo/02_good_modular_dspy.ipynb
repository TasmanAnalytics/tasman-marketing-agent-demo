{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Demo: Modular DSPy Agent Architecture\n",
    "\n",
    "## Production-Grade Analytics with Semantic Layer\n",
    "\n",
    "This notebook demonstrates the **correct** way to build LLM-powered analytics:\n",
    "\n",
    "### Architecture Principles\n",
    "\n",
    "1. **Semantic Layer First**: All metrics defined in `config/semantic.yml` with tested SQL\n",
    "2. **Modular Agents**: Small, focused, testable components with clear contracts\n",
    "3. **Local-First Logic**: Deterministic rules wherever possible; LLM only for ambiguity\n",
    "4. **DSPy Signatures**: Structured prompts that are declarative and optimizable\n",
    "5. **Observability**: Every decision logged with run_id, timings, and provenance\n",
    "\n",
    "### Business Question\n",
    "\n",
    "\"Which channel mix change is most likely to improve CAC next month, given a recent anomaly in referral traffic?\"\n",
    "\n",
    "### Agent Pipeline\n",
    "\n",
    "```\n",
    "Question → Triage → Text-to-Semantic → Metric Compilation → Execution → \n",
    "          Hypothesis Simulation → Narration → Observability\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bootstrap: Environment and Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Standard libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Our utilities\n",
    "from utils import (\n",
    "    load_environment,\n",
    "    get_db_connection,\n",
    "    validate_schema,\n",
    "    SemanticLayer,\n",
    "    plot_channel_metric,\n",
    "    RunRecord\n",
    ")\n",
    "\n",
    "# DSPy and OpenAI\n",
    "import dspy\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment\n",
    "env_config = load_environment()\n",
    "print(\"Environment loaded:\")\n",
    "print(f\"  Model: {env_config['OPENAI_MODEL']}\")\n",
    "print(f\"  DB Path: {env_config['DB_PATH']}\")\n",
    "print(f\"  API Key: {env_config['OPENAI_API_KEY'][:8]}...\")\n",
    "\n",
    "# Configure OpenAI\n",
    "openai_client = OpenAI(api_key=env_config['OPENAI_API_KEY'])\n",
    "\n",
    "# Configure DSPy\n",
    "lm = dspy.LM(f\"openai/{env_config['OPENAI_MODEL']}\", api_key=env_config['OPENAI_API_KEY'])\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "print(\"\\n✓ LLM configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database (read-only mode)\n",
    "db_path = env_config['DB_PATH']\n",
    "conn = get_db_connection(db_path, read_only=True)\n",
    "print(f\"✓ Connected to database: {db_path}\")\n",
    "\n",
    "# Validate schema\n",
    "schema_validation = validate_schema(conn)\n",
    "print(f\"\\n✓ Schema validation passed\")\n",
    "print(f\"  Tables found: {len(schema_validation['tables'])}\")\n",
    "for table in sorted(schema_validation['tables']):\n",
    "    print(f\"    - {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set defaults for the session\n",
    "DEFAULT_WINDOW_DAYS = 90\n",
    "DEFAULT_LIMIT = 1000\n",
    "OFFLINE_MODE = False  # Set to True to disable LLM calls for rehearsals\n",
    "\n",
    "print(\"Session defaults:\")\n",
    "print(f\"  Window: {DEFAULT_WINDOW_DAYS} days\")\n",
    "print(f\"  Limit: {DEFAULT_LIMIT} rows\")\n",
    "print(f\"  Offline mode: {OFFLINE_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic Catalogue\n",
    "\n",
    "Load the semantic layer that defines:\n",
    "- Canonical dimensions (channel, campaign_name, device, region)\n",
    "- Safe base queries (spend, conversions, revenue by channel)\n",
    "- Derived metrics (ROAS, CAC by channel)\n",
    "- Join rules (revenue attribution path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load semantic layer\n",
    "semantic = SemanticLayer('../config/semantic.yml')\n",
    "print(\"✓ Semantic layer loaded\")\n",
    "print(f\"  Spec version: {semantic.config.get('version')}\")\n",
    "print(f\"  Spec hash: {semantic.spec_hash}\")\n",
    "print(f\"\\n{semantic.describe_catalogue()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show key metric definitions\n",
    "print(\"\\nCanonical Metric Definitions:\")\n",
    "print(\"=\" * 60)\n",
    "for metric, formula in semantic.get_metric_definitions().items():\n",
    "    print(f\"  {metric:15} = {formula}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show join rules (critical for correctness)\n",
    "print(\"\\nJoin Rules (Revenue Attribution):\")\n",
    "print(\"=\" * 60)\n",
    "join_rules = semantic.get_join_rules()\n",
    "revenue_rule = join_rules.get('revenue_attribution', {})\n",
    "print(f\"Rule: {revenue_rule.get('description')}\")\n",
    "print(\"Path:\")\n",
    "for step in revenue_rule.get('path', []):\n",
    "    print(f\"  → {step}\")\n",
    "print(\"\\n⚠️ This is enforced in semantic.yml queries; LLM cannot override it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Architecture with DSPy\n",
    "\n",
    "Define agents as **DSPy signatures** with clear input/output contracts.\n",
    "\n",
    "### Agent Roles\n",
    "\n",
    "1. **TriageAgent**: Classify question type (search vs analysis)\n",
    "2. **TextToSemanticAgent**: Map natural language to semantic request\n",
    "3. **MetricRunner**: Compile and execute queries (deterministic)\n",
    "4. **HypothesisAgent**: Simulate budget scenarios (deterministic)\n",
    "5. **NarratorAgent**: Generate decision memo with constraints\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "- **Local rules first**: Use keywords/templates before LLM\n",
    "- **Fallback only**: LLM called only when confidence < threshold\n",
    "- **Validated outputs**: All LLM outputs validated against semantic layer\n",
    "- **Observable**: Every decision logged with confidence and reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy Signatures\n",
    "\n",
    "class TriageSignature(dspy.Signature):\n",
    "    \"\"\"Classify a user question as search or analysis.\"\"\"\n",
    "    question: str = dspy.InputField(desc=\"User's business question\")\n",
    "    mode: str = dspy.OutputField(desc=\"Either 'search' or 'analysis'\")\n",
    "    confidence: float = dspy.OutputField(desc=\"Confidence score 0-1\")\n",
    "    reason: str = dspy.OutputField(desc=\"Brief explanation of classification\")\n",
    "\n",
    "\n",
    "class TextToSemanticSignature(dspy.Signature):\n",
    "    \"\"\"Map natural language question to semantic request.\"\"\"\n",
    "    question: str = dspy.InputField(desc=\"User's business question\")\n",
    "    available_metrics: str = dspy.InputField(desc=\"List of available metrics\")\n",
    "    available_dimensions: str = dspy.InputField(desc=\"List of available dimensions\")\n",
    "    metric: str = dspy.OutputField(desc=\"Selected metric name\")\n",
    "    dimensions: str = dspy.OutputField(desc=\"Comma-separated dimension names\")\n",
    "    filters: str = dspy.OutputField(desc=\"Optional filters as JSON\")\n",
    "    window_days: int = dspy.OutputField(desc=\"Time window in days\")\n",
    "\n",
    "\n",
    "class NarratorSignature(dspy.Signature):\n",
    "    \"\"\"Generate concise decision memo from analysis results.\"\"\"\n",
    "    question: str = dspy.InputField(desc=\"Original business question\")\n",
    "    metrics_used: str = dspy.InputField(desc=\"Metrics that were queried\")\n",
    "    key_findings: str = dspy.InputField(desc=\"Key data findings\")\n",
    "    recommendation: str = dspy.InputField(desc=\"Proposed action with confidence interval\")\n",
    "    memo: str = dspy.OutputField(desc=\"Decision memo (max 150 words) with risks and next steps\")\n",
    "\n",
    "\n",
    "print(\"✓ DSPy signatures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Implementations\n",
    "\n",
    "class TriageAgent:\n",
    "    \"\"\"Classify questions as search or analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, offline_mode: bool = False):\n",
    "        self.offline_mode = offline_mode\n",
    "        self.predictor = dspy.Predict(TriageSignature)\n",
    "        \n",
    "        # Local keyword rules\n",
    "        self.analysis_keywords = [\n",
    "            'cac', 'roas', 'improve', 'optimize', 'compare', 'which',\n",
    "            'recommend', 'should', 'best', 'worst', 'trend', 'anomaly'\n",
    "        ]\n",
    "        self.search_keywords = [\n",
    "            'what is', 'show me', 'list', 'find', 'get', 'display'\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, question: str) -> Dict[str, Any]:\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Try local rules first\n",
    "        analysis_score = sum(1 for kw in self.analysis_keywords if kw in question_lower)\n",
    "        search_score = sum(1 for kw in self.search_keywords if kw in question_lower)\n",
    "        \n",
    "        if analysis_score > search_score and analysis_score >= 2:\n",
    "            return {\n",
    "                'mode': 'analysis',\n",
    "                'confidence': 0.9,\n",
    "                'reason': f'Matched {analysis_score} analysis keywords',\n",
    "                'method': 'local_rules'\n",
    "            }\n",
    "        elif search_score > analysis_score and search_score >= 2:\n",
    "            return {\n",
    "                'mode': 'search',\n",
    "                'confidence': 0.9,\n",
    "                'reason': f'Matched {search_score} search keywords',\n",
    "                'method': 'local_rules'\n",
    "            }\n",
    "        \n",
    "        # Fallback to LLM\n",
    "        if self.offline_mode:\n",
    "            return {\n",
    "                'mode': 'analysis',\n",
    "                'confidence': 0.5,\n",
    "                'reason': 'Default in offline mode',\n",
    "                'method': 'offline_default'\n",
    "            }\n",
    "        \n",
    "        result = self.predictor(question=question)\n",
    "        return {\n",
    "            'mode': result.mode,\n",
    "            'confidence': float(result.confidence),\n",
    "            'reason': result.reason,\n",
    "            'method': 'dspy_fallback'\n",
    "        }\n",
    "\n",
    "\n",
    "class TextToSemanticAgent:\n",
    "    \"\"\"Map natural language to semantic request.\"\"\"\n",
    "    \n",
    "    def __init__(self, semantic_layer: SemanticLayer, offline_mode: bool = False):\n",
    "        self.semantic = semantic_layer\n",
    "        self.offline_mode = offline_mode\n",
    "        self.predictor = dspy.Predict(TextToSemanticSignature)\n",
    "        \n",
    "        # Template mappings (utterance → semantic request)\n",
    "        self.templates = {\n",
    "            'cac by channel': {\n",
    "                'metric': 'cac_by_channel',\n",
    "                'dimensions': ['channel'],\n",
    "                'window_days': 90\n",
    "            },\n",
    "            'roas by channel': {\n",
    "                'metric': 'roas_by_channel',\n",
    "                'dimensions': ['channel'],\n",
    "                'window_days': 90\n",
    "            },\n",
    "            'channel performance': {\n",
    "                'metric': 'roas_by_channel',\n",
    "                'dimensions': ['channel'],\n",
    "                'window_days': 90\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def __call__(self, question: str) -> Dict[str, Any]:\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Try template matching first\n",
    "        for template_key, semantic_req in self.templates.items():\n",
    "            if template_key in question_lower:\n",
    "                return {\n",
    "                    'metric': semantic_req['metric'],\n",
    "                    'dimensions': semantic_req['dimensions'],\n",
    "                    'filters': {},\n",
    "                    'window_days': semantic_req['window_days'],\n",
    "                    'method': 'template_match',\n",
    "                    'matched_template': template_key\n",
    "                }\n",
    "        \n",
    "        # For complex questions, we may need multiple metrics\n",
    "        # Check if question mentions CAC optimization\n",
    "        if 'cac' in question_lower and ('improve' in question_lower or 'optimize' in question_lower):\n",
    "            # Need both CAC and ROAS for optimization\n",
    "            return {\n",
    "                'metric': 'cac_by_channel',  # Primary metric\n",
    "                'secondary_metrics': ['roas_by_channel'],\n",
    "                'dimensions': ['channel'],\n",
    "                'filters': {},\n",
    "                'window_days': 90,\n",
    "                'method': 'pattern_match',\n",
    "                'reason': 'CAC optimization requires both CAC and ROAS by channel'\n",
    "            }\n",
    "        \n",
    "        # Fallback to LLM with constraints\n",
    "        if self.offline_mode:\n",
    "            return {\n",
    "                'metric': 'cac_by_channel',\n",
    "                'dimensions': ['channel'],\n",
    "                'filters': {},\n",
    "                'window_days': 90,\n",
    "                'method': 'offline_default'\n",
    "            }\n",
    "        \n",
    "        available_metrics = ', '.join(self.semantic.list_available_metrics())\n",
    "        available_dimensions = ', '.join(self.semantic.get_dimension_names())\n",
    "        \n",
    "        result = self.predictor(\n",
    "            question=question,\n",
    "            available_metrics=available_metrics,\n",
    "            available_dimensions=available_dimensions\n",
    "        )\n",
    "        \n",
    "        # Validate against semantic layer\n",
    "        metric = result.metric.strip()\n",
    "        if metric not in self.semantic.list_available_metrics():\n",
    "            raise ValueError(\n",
    "                f\"LLM proposed unknown metric '{metric}'. \"\n",
    "                f\"Available: {available_metrics}\"\n",
    "            )\n",
    "        \n",
    "        dimensions = [d.strip() for d in result.dimensions.split(',')]\n",
    "        for dim in dimensions:\n",
    "            if not self.semantic.validate_dimension(dim):\n",
    "                raise ValueError(\n",
    "                    f\"LLM proposed unknown dimension '{dim}'. \"\n",
    "                    f\"Available: {available_dimensions}\"\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            'metric': metric,\n",
    "            'dimensions': dimensions,\n",
    "            'filters': {},\n",
    "            'window_days': int(result.window_days),\n",
    "            'method': 'dspy_constrained'\n",
    "        }\n",
    "\n",
    "\n",
    "class MetricRunner:\n",
    "    \"\"\"Compile and execute queries from semantic layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_conn, semantic_layer: SemanticLayer):\n",
    "        self.conn = db_conn\n",
    "        self.semantic = semantic_layer\n",
    "    \n",
    "    def __call__(self, metric: str, window_days: int, limit: int = 1000) -> Dict[str, Any]:\n",
    "        # Compile query from semantic layer\n",
    "        query_info = self.semantic.compile_query(metric, window_days, limit)\n",
    "        \n",
    "        # Execute with timing\n",
    "        start_time = time.time()\n",
    "        result_df = self.conn.execute(query_info['sql']).df()\n",
    "        elapsed_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Sanity checks\n",
    "        row_count = len(result_df)\n",
    "        \n",
    "        return {\n",
    "            'query_info': query_info,\n",
    "            'df': result_df,\n",
    "            'elapsed_ms': elapsed_ms,\n",
    "            'row_count': row_count\n",
    "        }\n",
    "\n",
    "\n",
    "class HypothesisAgent:\n",
    "    \"\"\"Simulate budget reallocation scenarios.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_bootstrap: int = 1000):\n",
    "        self.n_bootstrap = n_bootstrap\n",
    "    \n",
    "    def __call__(self, cac_df: pd.DataFrame, spend_col: str = 'spend',\n",
    "                 cac_col: str = 'cac', channel_col: str = 'channel',\n",
    "                 shift_percentage: float = 5.0) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Simulate budget shift from worst to best CAC channel.\n",
    "        \n",
    "        Args:\n",
    "            cac_df: DataFrame with CAC by channel\n",
    "            spend_col: Column name for spend\n",
    "            cac_col: Column name for CAC\n",
    "            channel_col: Column name for channel\n",
    "            shift_percentage: Percentage points to shift (default 5pp)\n",
    "        \n",
    "        Returns:\n",
    "            Dict with scenario results and confidence interval\n",
    "        \"\"\"\n",
    "        # Filter valid CAC values\n",
    "        valid_df = cac_df[cac_df[cac_col].notna()].copy()\n",
    "        \n",
    "        if len(valid_df) < 2:\n",
    "            raise ValueError(\"Need at least 2 channels with valid CAC for hypothesis testing\")\n",
    "        \n",
    "        # Sort by CAC (ascending = better)\n",
    "        valid_df = valid_df.sort_values(cac_col)\n",
    "        \n",
    "        # Identify best and worst channels\n",
    "        best_channel = valid_df.iloc[0][channel_col]\n",
    "        worst_channel = valid_df.iloc[-1][channel_col]\n",
    "        best_cac = valid_df.iloc[0][cac_col]\n",
    "        worst_cac = valid_df.iloc[-1][cac_col]\n",
    "        \n",
    "        # Current blended CAC\n",
    "        total_spend = valid_df[spend_col].sum()\n",
    "        weights = valid_df[spend_col] / total_spend\n",
    "        current_blended_cac = (weights * valid_df[cac_col]).sum()\n",
    "        \n",
    "        # Simulate shift: move 5pp from worst to best\n",
    "        shift_fraction = shift_percentage / 100.0\n",
    "        new_weights = weights.copy()\n",
    "        \n",
    "        best_idx = valid_df[valid_df[channel_col] == best_channel].index[0]\n",
    "        worst_idx = valid_df[valid_df[channel_col] == worst_channel].index[0]\n",
    "        \n",
    "        new_weights[worst_idx] -= shift_fraction\n",
    "        new_weights[best_idx] += shift_fraction\n",
    "        \n",
    "        # Projected CAC\n",
    "        projected_cac = (new_weights * valid_df[cac_col]).sum()\n",
    "        delta_cac = projected_cac - current_blended_cac\n",
    "        \n",
    "        # Bootstrap confidence interval\n",
    "        bootstrap_deltas = []\n",
    "        for _ in range(self.n_bootstrap):\n",
    "            # Resample channels with replacement\n",
    "            sample_df = valid_df.sample(n=len(valid_df), replace=True)\n",
    "            sample_weights = sample_df[spend_col] / sample_df[spend_col].sum()\n",
    "            \n",
    "            current_sample = (sample_weights * sample_df[cac_col]).sum()\n",
    "            \n",
    "            # Apply same shift\n",
    "            new_sample_weights = sample_weights.copy()\n",
    "            sample_best_idx = sample_df[sample_df[cac_col] == sample_df[cac_col].min()].index[0]\n",
    "            sample_worst_idx = sample_df[sample_df[cac_col] == sample_df[cac_col].max()].index[0]\n",
    "            \n",
    "            new_sample_weights[sample_worst_idx] -= shift_fraction\n",
    "            new_sample_weights[sample_best_idx] += shift_fraction\n",
    "            \n",
    "            projected_sample = (new_sample_weights * sample_df[cac_col]).sum()\n",
    "            bootstrap_deltas.append(projected_sample - current_sample)\n",
    "        \n",
    "        # 95% CI\n",
    "        ci_lower = current_blended_cac + np.percentile(bootstrap_deltas, 2.5)\n",
    "        ci_upper = current_blended_cac + np.percentile(bootstrap_deltas, 97.5)\n",
    "        \n",
    "        return {\n",
    "            'best_channel': best_channel,\n",
    "            'best_cac': float(best_cac),\n",
    "            'worst_channel': worst_channel,\n",
    "            'worst_cac': float(worst_cac),\n",
    "            'current_blended_cac': float(current_blended_cac),\n",
    "            'shift_percentage': shift_percentage,\n",
    "            'projected_blended_cac': float(projected_cac),\n",
    "            'delta_cac': float(delta_cac),\n",
    "            'ci_lower': float(ci_lower),\n",
    "            'ci_upper': float(ci_upper),\n",
    "            'n_bootstrap': self.n_bootstrap,\n",
    "            'recommendation': f\"Shift {shift_percentage}pp budget from {worst_channel} to {best_channel}\"\n",
    "        }\n",
    "\n",
    "\n",
    "class NarratorAgent:\n",
    "    \"\"\"Generate decision memo from analysis results.\"\"\"\n",
    "    \n",
    "    def __init__(self, offline_mode: bool = False):\n",
    "        self.offline_mode = offline_mode\n",
    "        self.predictor = dspy.Predict(NarratorSignature)\n",
    "    \n",
    "    def __call__(self, question: str, metrics_used: List[str],\n",
    "                 key_findings: str, recommendation: str) -> Dict[str, str]:\n",
    "        \n",
    "        if self.offline_mode:\n",
    "            memo = f\"\"\"Based on analysis of {', '.join(metrics_used)}, {key_findings}. \n",
    "            {recommendation}. \n",
    "            Risks: (1) Data quality assumptions, (2) Attribution model simplifications. \n",
    "            Next steps: (1) Pilot small shift, (2) Monitor for 14 days.\"\"\"\n",
    "            return {'memo': memo, 'word_count': len(memo.split())}\n",
    "        \n",
    "        result = self.predictor(\n",
    "            question=question,\n",
    "            metrics_used=', '.join(metrics_used),\n",
    "            key_findings=key_findings,\n",
    "            recommendation=recommendation\n",
    "        )\n",
    "        \n",
    "        memo = result.memo\n",
    "        word_count = len(memo.split())\n",
    "        \n",
    "        # Validate constraints\n",
    "        if word_count > 200:\n",
    "            # Truncate to ~150 words\n",
    "            words = memo.split()[:150]\n",
    "            memo = ' '.join(words) + '...'\n",
    "            word_count = 150\n",
    "        \n",
    "        # Check that it references at least one metric\n",
    "        memo_lower = memo.lower()\n",
    "        metric_referenced = any(m.lower() in memo_lower for m in metrics_used)\n",
    "        if not metric_referenced:\n",
    "            memo = f\"[Metrics: {', '.join(metrics_used)}] \" + memo\n",
    "        \n",
    "        return {\n",
    "            'memo': memo,\n",
    "            'word_count': word_count,\n",
    "            'constraints_met': word_count <= 200 and metric_referenced\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ Agent implementations complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents\n",
    "triage_agent = TriageAgent(offline_mode=OFFLINE_MODE)\n",
    "text_to_semantic_agent = TextToSemanticAgent(semantic, offline_mode=OFFLINE_MODE)\n",
    "metric_runner = MetricRunner(conn, semantic)\n",
    "hypothesis_agent = HypothesisAgent(n_bootstrap=1000)\n",
    "narrator_agent = NarratorAgent(offline_mode=OFFLINE_MODE)\n",
    "\n",
    "print(\"✓ All agents initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize observability\n",
    "run_record = RunRecord(\n",
    "    model_name=env_config['OPENAI_MODEL'],\n",
    "    semantic_spec_hash=semantic.spec_hash\n",
    ")\n",
    "\n",
    "print(f\"✓ Run record initialized: {run_record.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Triage: Classify Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business question\n",
    "business_question = \"\"\"Which channel mix change is most likely to improve CAC next month, \n",
    "given a recent anomaly in referral traffic?\"\"\"\n",
    "\n",
    "print(f\"Question: {business_question}\")\n",
    "print(\"\\nRunning triage...\")\n",
    "\n",
    "triage_result = triage_agent(business_question)\n",
    "run_record.record_triage(triage_result)\n",
    "\n",
    "print(\"\\nTriage Result:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in triage_result.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "assert triage_result['mode'] == 'analysis', \"Expected analysis mode for this question\"\n",
    "print(\"\\n✓ Triage complete: mode=analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Mapping: NL → Structured Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mapping question to semantic request...\")\n",
    "\n",
    "semantic_request = text_to_semantic_agent(business_question)\n",
    "run_record.record_semantic_request(semantic_request)\n",
    "\n",
    "print(\"\\nSemantic Request:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in semantic_request.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n✓ Semantic mapping complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metric Compilation and Execution\n",
    "\n",
    "Compile SQL from semantic layer (no LLM) and execute safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need both CAC and ROAS by channel for informed decision\n",
    "metrics_to_run = ['cac_by_channel', 'roas_by_channel']\n",
    "window_days = semantic_request.get('window_days', DEFAULT_WINDOW_DAYS)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for metric in metrics_to_run:\n",
    "    print(f\"\\nCompiling and executing: {metric}\")\n",
    "    result = metric_runner(metric, window_days)\n",
    "    \n",
    "    # Record in observability\n",
    "    run_record.record_query(result['query_info'])\n",
    "    run_record.record_execution(\n",
    "        result['query_info']['query_id'],\n",
    "        result['elapsed_ms'],\n",
    "        result['row_count']\n",
    "    )\n",
    "    \n",
    "    results[metric] = result\n",
    "    \n",
    "    print(f\"  Query ID: {result['query_info']['query_id']}\")\n",
    "    print(f\"  Execution time: {result['elapsed_ms']:.2f} ms\")\n",
    "    print(f\"  Rows returned: {result['row_count']}\")\n",
    "\n",
    "print(\"\\n✓ All metrics executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display CAC by channel\n",
    "cac_df = results['cac_by_channel']['df']\n",
    "print(\"\\nCAC by Channel:\")\n",
    "print(\"=\" * 60)\n",
    "print(cac_df.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig1 = plot_channel_metric(\n",
    "    cac_df,\n",
    "    channel_col='channel',\n",
    "    metric_col='cac',\n",
    "    title=f'CAC by Channel (Last {window_days} Days)',\n",
    "    ylabel='CAC ($)'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "output_path = Path('./outputs')\n",
    "output_path.mkdir(exist_ok=True)\n",
    "fig1.savefig(output_path / f\"{run_record.run_id}_cac_by_channel.png\", dpi=100, bbox_inches='tight')\n",
    "run_record.add_artifact('chart', str(output_path / f\"{run_record.run_id}_cac_by_channel.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ROAS by channel\n",
    "roas_df = results['roas_by_channel']['df']\n",
    "print(\"\\nROAS by Channel:\")\n",
    "print(\"=\" * 60)\n",
    "print(roas_df.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig2 = plot_channel_metric(\n",
    "    roas_df,\n",
    "    channel_col='channel',\n",
    "    metric_col='roas',\n",
    "    title=f'ROAS by Channel (Last {window_days} Days)',\n",
    "    ylabel='ROAS (X)'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "fig2.savefig(output_path / f\"{run_record.run_id}_roas_by_channel.png\", dpi=100, bbox_inches='tight')\n",
    "run_record.add_artifact('chart', str(output_path / f\"{run_record.run_id}_roas_by_channel.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hypothesis Simulation\n",
    "\n",
    "Test budget reallocation: shift 5pp from worst to best CAC channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running hypothesis simulation...\\n\")\n",
    "\n",
    "hypothesis_result = hypothesis_agent(cac_df)\n",
    "\n",
    "# Record\n",
    "hypothesis_params = {\n",
    "    'shift_percentage': hypothesis_result['shift_percentage'],\n",
    "    'n_bootstrap': hypothesis_result['n_bootstrap']\n",
    "}\n",
    "run_record.record_hypothesis(hypothesis_params, hypothesis_result)\n",
    "\n",
    "print(\"Hypothesis Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best channel: {hypothesis_result['best_channel']} (CAC: ${hypothesis_result['best_cac']:.2f})\")\n",
    "print(f\"Worst channel: {hypothesis_result['worst_channel']} (CAC: ${hypothesis_result['worst_cac']:.2f})\")\n",
    "print(f\"\\nCurrent blended CAC: ${hypothesis_result['current_blended_cac']:.2f}\")\n",
    "print(f\"Projected blended CAC: ${hypothesis_result['projected_blended_cac']:.2f}\")\n",
    "print(f\"Expected delta: ${hypothesis_result['delta_cac']:.2f}\")\n",
    "print(f\"\\n95% Confidence Interval: [${hypothesis_result['ci_lower']:.2f}, ${hypothesis_result['ci_upper']:.2f}]\")\n",
    "print(f\"\\nRecommendation: {hypothesis_result['recommendation']}\")\n",
    "\n",
    "print(\"\\n✓ Hypothesis simulation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hypothesis\n",
    "from utils.plotting import plot_hypothesis_comparison\n",
    "\n",
    "fig3 = plot_hypothesis_comparison(\n",
    "    current_cac=hypothesis_result['current_blended_cac'],\n",
    "    projected_cac=hypothesis_result['projected_blended_cac'],\n",
    "    ci_lower=hypothesis_result['ci_lower'],\n",
    "    ci_upper=hypothesis_result['ci_upper'],\n",
    "    title='Projected CAC Impact with 95% CI'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "fig3.savefig(output_path / f\"{run_record.run_id}_hypothesis.png\", dpi=100, bbox_inches='tight')\n",
    "run_record.add_artifact('chart', str(output_path / f\"{run_record.run_id}_hypothesis.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Narration: Generate Decision Memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for narrator\n",
    "metrics_used = [r['query_info']['query_id'] for r in results.values()]\n",
    "\n",
    "key_findings = f\"\"\"\n",
    "Analyzed {len(cac_df)} channels over {window_days} days. \n",
    "Best CAC: {hypothesis_result['best_channel']} at ${hypothesis_result['best_cac']:.2f}. \n",
    "Worst CAC: {hypothesis_result['worst_channel']} at ${hypothesis_result['worst_cac']:.2f}. \n",
    "Current blended CAC: ${hypothesis_result['current_blended_cac']:.2f}.\n",
    "\"\"\".strip()\n",
    "\n",
    "recommendation_text = f\"\"\"\n",
    "Shift {hypothesis_result['shift_percentage']}pp budget from {hypothesis_result['worst_channel']} \n",
    "to {hypothesis_result['best_channel']}. \n",
    "Projected CAC: ${hypothesis_result['projected_blended_cac']:.2f} \n",
    "(95% CI: [${hypothesis_result['ci_lower']:.2f}, ${hypothesis_result['ci_upper']:.2f}]).\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"Generating decision memo...\\n\")\n",
    "\n",
    "narration_result = narrator_agent(\n",
    "    question=business_question,\n",
    "    metrics_used=metrics_used,\n",
    "    key_findings=key_findings,\n",
    "    recommendation=recommendation_text\n",
    ")\n",
    "\n",
    "# Record\n",
    "run_record.record_narration(narration_result['memo'])\n",
    "\n",
    "print(\"Decision Memo:\")\n",
    "print(\"=\" * 60)\n",
    "print(narration_result['memo'])\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Word count: {narration_result['word_count']}\")\n",
    "print(f\"Constraints met: {narration_result.get('constraints_met', 'N/A')}\")\n",
    "\n",
    "print(\"\\n✓ Narration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Observability: Run Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize run record\n",
    "run_record.finalize()\n",
    "\n",
    "# Display\n",
    "print(\"Run Record:\")\n",
    "print(\"=\" * 60)\n",
    "print(run_record.to_json())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Save to file\n",
    "saved_path = run_record.save(output_dir='./outputs')\n",
    "print(f\"\\n✓ Run record saved: {saved_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + run_record.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inline Tests and Smoke Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running inline tests...\\n\")\n",
    "\n",
    "# Test 1: Schema presence\n",
    "try:\n",
    "    schema_check = validate_schema(conn)\n",
    "    run_record.record_test('schema_validation', True, 'All required tables and columns present')\n",
    "    print(\"✓ Test 1: Schema validation passed\")\n",
    "except Exception as e:\n",
    "    run_record.record_test('schema_validation', False, str(e))\n",
    "    print(f\"✗ Test 1: Schema validation failed: {e}\")\n",
    "\n",
    "# Test 2: SQL compilation for derived metrics\n",
    "try:\n",
    "    for metric in ['roas_by_channel', 'cac_by_channel']:\n",
    "        compiled = semantic.compile_query(metric, DEFAULT_WINDOW_DAYS)\n",
    "        assert 'sql' in compiled\n",
    "        assert len(compiled['sql']) > 0\n",
    "    run_record.record_test('sql_compilation', True, 'Both derived metrics compile successfully')\n",
    "    print(\"✓ Test 2: SQL compilation passed\")\n",
    "except Exception as e:\n",
    "    run_record.record_test('sql_compilation', False, str(e))\n",
    "    print(f\"✗ Test 2: SQL compilation failed: {e}\")\n",
    "\n",
    "# Test 3: No cartesian explosion (channel count sanity)\n",
    "try:\n",
    "    expected_max_channels = 10  # Reasonable upper bound\n",
    "    for metric, result in results.items():\n",
    "        row_count = result['row_count']\n",
    "        assert row_count <= expected_max_channels, f\"{metric} returned {row_count} rows (expected <= {expected_max_channels})\"\n",
    "    run_record.record_test('no_cartesian_explosion', True, f'All queries returned <= {expected_max_channels} rows')\n",
    "    print(\"✓ Test 3: No cartesian explosion\")\n",
    "except Exception as e:\n",
    "    run_record.record_test('no_cartesian_explosion', False, str(e))\n",
    "    print(f\"✗ Test 3: Cartesian explosion detected: {e}\")\n",
    "\n",
    "# Test 4: Triage accuracy on canned queries\n",
    "try:\n",
    "    test_cases = [\n",
    "        ('What is the CAC by channel?', 'analysis'),\n",
    "        ('Show me campaign performance', 'analysis'),\n",
    "        ('Which channel should I optimize?', 'analysis'),\n",
    "        ('List all campaigns', 'search'),\n",
    "    ]\n",
    "    correct = 0\n",
    "    for query, expected_mode in test_cases:\n",
    "        result = triage_agent(query)\n",
    "        if result['mode'] == expected_mode:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_cases)\n",
    "    threshold = 0.75\n",
    "    passed = accuracy >= threshold\n",
    "    run_record.record_test('triage_accuracy', passed, f'Accuracy: {accuracy:.2%} (threshold: {threshold:.2%})')\n",
    "    if passed:\n",
    "        print(f\"✓ Test 4: Triage accuracy {accuracy:.2%} >= {threshold:.2%}\")\n",
    "    else:\n",
    "        print(f\"✗ Test 4: Triage accuracy {accuracy:.2%} < {threshold:.2%}\")\n",
    "except Exception as e:\n",
    "    run_record.record_test('triage_accuracy', False, str(e))\n",
    "    print(f\"✗ Test 4: Triage accuracy test failed: {e}\")\n",
    "\n",
    "# Test 5: Narration lint (length and metric reference)\n",
    "try:\n",
    "    memo = narration_result['memo']\n",
    "    word_count = narration_result['word_count']\n",
    "    \n",
    "    # Check word count\n",
    "    assert word_count <= 200, f\"Narration too long: {word_count} words\"\n",
    "    \n",
    "    # Check metric reference\n",
    "    memo_lower = memo.lower()\n",
    "    metric_referenced = any(m.lower() in memo_lower for m in metrics_used) or 'cac' in memo_lower or 'roas' in memo_lower\n",
    "    assert metric_referenced, \"Narration does not reference any metrics\"\n",
    "    \n",
    "    run_record.record_test('narration_lint', True, f'Word count: {word_count}, metric referenced: {metric_referenced}')\n",
    "    print(f\"✓ Test 5: Narration lint passed ({word_count} words, metric referenced)\")\n",
    "except AssertionError as e:\n",
    "    run_record.record_test('narration_lint', False, str(e))\n",
    "    print(f\"✗ Test 5: Narration lint failed: {e}\")\n",
    "except Exception as e:\n",
    "    run_record.record_test('narration_lint', False, str(e))\n",
    "    print(f\"✗ Test 5: Narration lint error: {e}\")\n",
    "\n",
    "# Summary\n",
    "total_tests = len(run_record.test_results)\n",
    "passed_tests = sum(1 for t in run_record.test_results.values() if t['passed'])\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Tests passed: {passed_tests}/{total_tests}\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What We Built\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "1. **Semantic Layer** (`config/semantic.yml`)\n",
    "   - Canonical metric definitions\n",
    "   - Safe, tested base queries\n",
    "   - Join rules enforced\n",
    "   - Versioned with hash\n",
    "\n",
    "2. **Modular Agents** (DSPy signatures)\n",
    "   - TriageAgent: Local rules + LLM fallback\n",
    "   - TextToSemanticAgent: Template matching + constrained LLM\n",
    "   - MetricRunner: Deterministic SQL compilation\n",
    "   - HypothesisAgent: Bootstrap confidence intervals\n",
    "   - NarratorAgent: Structured output with constraints\n",
    "\n",
    "3. **Observability**\n",
    "   - Unique run_id for every execution\n",
    "   - All decisions logged with provenance\n",
    "   - Timings, row counts, SQL IDs tracked\n",
    "   - Artifacts (charts, data) referenced\n",
    "   - Test results recorded\n",
    "\n",
    "### Key Differences from Bad Demo\n",
    "\n",
    "| Bad Demo | Good Demo |\n",
    "|----------|----------|\n",
    "| One-shot LLM on raw data | Modular agents with semantic layer |\n",
    "| No validation | Schema validation, sanity checks, tests |\n",
    "| Wrong joins, time windows | Enforced in semantic.yml |\n",
    "| No confidence intervals | Bootstrap CI for projections |\n",
    "| Overconfident narratives | Constrained output with risks |\n",
    "| No reproducibility | Full observability with run_id |\n",
    "\n",
    "### Production-Ready Features\n",
    "\n",
    "- ✓ All data access through semantic layer\n",
    "- ✓ LLM used only for ambiguity resolution\n",
    "- ✓ Deterministic logic wherever possible\n",
    "- ✓ Comprehensive testing and validation\n",
    "- ✓ Full observability and reproducibility\n",
    "- ✓ Quantified uncertainty (confidence intervals)\n",
    "- ✓ Clear decision memos with risks and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "conn.close()\n",
    "print(\"\\n✓ Demo complete. Database connection closed.\")\n",
    "print(f\"\\nAll artifacts saved to: {output_path.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
